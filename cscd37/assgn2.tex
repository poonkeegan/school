\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{pgfplotstable}
\usepackage{longtable}
\usepackage{algorithm2e}
\graphicspath{ {images/} }
\pagestyle{empty}
\fancyhf{}
\cfoot{\thepage}

\newtheorem*{lemma}{Lemma}
\newcommand{\twonorm}[1]{\| #1 \|_2}
\newcommand{\rdim}[2]{\mathbb{R}^{#1 \times #2}}
\newcommand{\R}{\mathbb{R}}
\lhead{CSCD37: Assignment \#1 }
\rhead{
Poon, Keegan\\
poonkeeg\\
1002423727}
\pagenumbering{gobble}
\renewcommand{\headrulewidth}{0pt}
\begin{document}

\thispagestyle{fancy}
\begin{enumerate}
    \item Recall the full-Newton algorithm for solving the nonlinear system $F(x) = 0; F,x\in \mathcal{R}^n:$

        \begin{algorithm}[H]
            \SetAlgoNoLine
            \SetEndCharOfAlgoLine{}
            \SetKwProg{Fn}{}{}{}
            \Fn{}{
                Generate an initial approximation $\hat x_0$ \;
                for $k = 0,1\dots$ until convergence\;
                \Indp compute $-F(\hat x_k)$ \;
                    compute $\frac{\partial F(\hat x_k)}{\partial x}$ \;
                    solve $\frac{\partial F(\hat x_k)}{\partial x} \Delta_k = -F(\hat x_k)$ \;
                    update $\hat x_{k+1} = \hat x_k + \Delta_k$ \;
                \Indm end for
            }
        \end{algorithm}
        \begin{enumerate}
            \item This algorithm is computationally expensive. Give a detailed analysis of the cost using standard big-oh notation. You may use results from CSCC37; e.e., the cost (measuring flops) of the LU-factorization of an $n \times n$ matrix is $(1/3)n^3 + \mathcal O (n^2)$.

            \item We briefly discussed in lecture the ``quasi-Newton algorithm" for solvling nonlinear systems. Modify the algorithm above to take advantage of the optimizations we discussed. You do not need to implement the modifications \dots pseudo-code will suffice. Be careful to discuss both flop optimizations and convergence issues (``X-test" and ``F-test" tolerance, maximum number of iterations, condition of Jacobian, how long to hold Jacobian fixed, etc.).
        \end{enumerate}
    \newpage
    \item In lecture we derived the divided-difference (Newton) form of the interpolating polynomial for the simple interpolation problem. This question will investigate how the Newton polynomial can be used for osculatory interpolation.
        \begin{enumerate}
            \item Prove that
                \[ \lim_{\substack{x_{i+j} \rightarrow x_i \\ 1 \leq j \leq k}}y[x_{i+k},x_{i+k-1},\dots,x_i] = \frac{y^{(k)}(x_i)}{k!} \]
                Provided $y \in \mathcal C^k$.

                \begin{proof}
                    This will be proven using induction on the elements of the divided difference, i.e. for an indexed set of elements $\{a_0, a_1, a_2, \dots, a_k\}$ it will be on $k$.
                    For the case $k = 0$, 
                    \[ \lim_{\substack{x_{i+j} \rightarrow x_i \\ 1 \leq j \leq k}}y[x_{i+j}] = \lim_{\substack{x_{i+j} \rightarrow x_i \\ 1 \leq j \leq k}}y(x_{i + j}) \overset{\text{cont}}{=} y (x_i) = \frac{y^{(0)}(x_i)}{1!} \]
                    For the inductive case, assume it holds for $n < k$
                    \begin{align*}
                        \lim_{\substack{x_{i+j} \rightarrow x_i \\ 1 \leq j \leq k}}y[x_{i+k},x_{i+k-1},\dots,x_i] &=
                        \lim_{\substack{x_{i+j} \rightarrow x_i \\ 1 \leq j \leq k}} \frac{y[x_{i+k},x_{i+k-1},\dots,x_{i+1}] - y[x_{i+k - 1},x_{i+k-1},\dots,x_{i}]}{x_{i+k} - x_i} \\
                        &= \lim_{\substack{x_{i+j} \rightarrow x_i \\ 1 \leq j \leq k}}\frac{y^{(k-1)}(x_{i+1})}{(x_{i+k} - x_i)(k-1)!} - \frac{y^{(k-1)}(x_{i})}{(x_{i+k} - x_i)(k-1)!}  \\
                        &= \frac{1}{(k-1)!}\lim_{\substack{x_{i+j} \rightarrow x_i \\ 1 \leq j \leq k}}\frac{y^{(k-1)}(x_{i+1}) - y^{(k-1)}(x_{i})}{x_{i+k} - x_i}\\
                    \end{align*}  

                \end{proof}   
            \item The result in (a) tells us that divided differences can be replaced with derivatives as data points coincide. Using this result, construct a divided difference table to find the coefficients of the Newton polynomial of degree 6 or less that satisfies the following interpolation conditions:
                \begin{align*}
                    &p(-1) = 4& &p(0) = 7& &p(1) = 28& &p(2) = 247& \\
                    & & &p'(0) = 6& &p'(1) = 56& & & \\
                    & & & & &p''(1) = 140& & & \\
                \end{align*}
            \item Use the Method of Undetermined Coefficients (i.e., as discusseds in lecture, construct and solve an appropriate Vandermonde system) to find the coefficients of the monomial-basis polynomial of degree 6 or less that satisfies the interpolation conditions specified in (b).

                You may use \textit{MatLab} for this question if you wish. Verify that you have obtained the same polynomial as in (b).
        \end{enumerate}
    \newpage
    \item Consider the function $y\in \mathcal C^{n+1}$ and the polynomial $p \in \mathcal P_n$ which satisfies
        \[ p^{(j)}(x_i) = y^{(j)}(x_i); \; j=0,\dots,j_i; \; i=0,\dots,k; \; \sum_{i=0}^k (j_i + 1) = n+1;\]
        with all of the $x_i$ distinct. The error in this polynomial interpolant is given by
        \begin{equation}
            E(x) = y(x) - p(x) = \frac{y^{(n+1)}(\xi)}{(n+1)!} (x-x_0)^{j_0+1}(x-x_1)^{j_1+1}\cdots(x-x_k)^{j_k+1}
        \end{equation}
        where $\xi \in $ span$\{x_0,\dots,x_k,x\}$ = $[\min\{x_0,\dots,x_k,x\},\max\{x_0,\dots,x_k,x\}]$ provided $y \in \mathcal C^{n+1}$ on span$\{x_0,\dots,x_k,x\}$.

        This is a fundemental formula in Numerical Aprroximation. The following is an outline of a possible derivation of (1). In this question you will expand this outline by proving certain key statements.

        If $x = x_i,\, i=0,\dots,k,$ then $y(x)-p(x) = 0$ since $p$ interpolates $y$ at these $k+1$ points. Also, $E(x_i) = 0$ in (1) since $(x_i-x_i)^{j_1+1} = 0$. Therefore, (1) holds when $x=x_i$.
        
        Now assume $x \not = x_i$ for any $i = 0,\dots,k$ and consider $x$ \textit{fixed}. Let $F(t) = y(t) - p(t) - CW(t)$ where $C = [y(x) - p(x)]/W(x)$ is a constant and
        \begin{equation}
            W(t) = (t-x_0)^{j_0 +1}(t-x_1)^{j_1+1}\cdots (t-x_k)^{j_k+1}
        \end{equation}
        is a polynomial of degree $n+1$. Clearly $F(x) = 0$, and also
        \begin{equation}
            F^{(j)}(x_1) = 0;\; j=0,\dots,j_i;\; i=0,\dots,k.
        \end{equation}

        Therefore, counting multiplicities, $F(t)$ has at least $n+2$ zeros in span$\{x_0,\dots,x_k,x\}$, which implies $F^{(n+1)}(t)$ has at least 1 zero in span$\{x_0,\dots,x_k,x\}$, or, in other words,

        \begin{equation}
            F^{(n+1)}(\xi) = 0, \xi \in \text{span}\{x_0,\dots,x_k,x\}.
        \end{equation}
        But

        \begin{equation}
            F^{(n+1)}(t) = \frac{d^{n+1}}{dt^{n+1}}[y(t) - p(t) - CW(t)] = y^{(n+1)}(t) - (n+1)!C.
        \end{equation}

        Therefore,
            \[ F^{(n+1)}(\xi) = 0 \implies y^{(n+1)}(\xi) - (n+1)!C = 0 \implies C = \frac{y^{(n+1)}(\xi)}{(n+1)!}. \] 
        But
            \[ C = \frac{y(x) - p(x)}{W(x)} \implies y(x) - p(x) = \frac{y^{(n+1)}(\xi)}{(n+1)!}W(x). \]
        Now for the statements you must prove:
        \begin{enumerate}
            \item Prove that $W(t)$ in (2) is a polynomial of degree $n+1$.
            \item Prove (3)
            \item Explain how (4) follows from $F(t)$ having at least $n+2$ zeros in span$\{x_0,\dots,x_k,x\}$.
            \item Prove (5).
        \end{enumerate}
    \newpage
    \item In lecture we proved that the roots of the Chebyshev polynomial
        \begin{equation}
            T_k(x) = \cos(k\cos^{-1}(x)), k=0,1,\dots
        \end{equation}
        are the optimal interpolation points on [-1,1].
        \begin{enumerate}
            \item Prove that (6) is a polynomial of degree $k$ for all $k \geq 0$.
                \begin{proof}
                    For the case of $k= 0,  1$, we have that 
                    \begin{align*}
                        \cos(k\arccos(x)) |_{k=0} &= \cos(0) = 1 \\
                        \cos(k\arccos(x)) |_{k=1} &= \cos(\arccos x) = x \\
                    \end{align*}
                    So these cases hold. Using induction, assume that $T_n(x)$ is a polynomial for $n < k$, then there are two inductive cases:

                    First $k = 2t$ is even
                    \begin{align*}
                        \cos(k\arccos(x)) &= \cos(2(t\arccos(x)))  \\
                        &= 2(\cos(t\arccos(x)))^2 - 1 \text{ [Double angle identity]}\\
                        &= 2(T_t(x))^2 - 1 
                    \end{align*}
                    This is again a polynomial since $T_t$ is one by IH, so this case holds.

                    Second $k = 2t + 1$ is odd
                    \begin{align*}
                        \cos(k\arccos(x)) &= \cos(2(t\arccos(x)) + \arccos(x)) \\
                        &\overset{\text{angle sum}}{=} \cos(2t\arccos(x))(\cos(\arccos(x))) - \sin(2t\arccos(x))(\sin(\arccos(x))) \\
                        &= xT_{2t}(x)- \sin(2t\arccos(x))(\sin(\arccos(x))) \\
                        &\overset{\text{double angle}}{=} xT_{2k}(x)- 2\cos(t\arccos(x))\sin(t\arccos(x))(\sin(\arccos(x))) \\
                        &= xT_{2t}(x)- 2T_t(x)\sin(t\arccos(x))(\sin(\arccos(x))) \\
                        &\overset{\text{angle prod}}{=} xT_{2k}(x)- T_t(x)(\cos((t-1)\arccos(x))-\cos((t+1)\arccos(x)))\\
                        &= xT_{2t}(x)- T_t(x)(T_{t-1}-T_{t+1})\\
                    \end{align*}
                    Which is again a polynomial all the lower degrees of $T$ are polynomials by IH, so both cases hold.
                \end{proof}
            \item Derive the leading coefficient of (6) (i.e., the coefficient of $x^k$).

                The coefficient is $2^{k-1}$ for $k \geq 1$ and $1$ for $k=0$.
                \begin{proof}
                    Again, using induction, the case of $1$ and $0$ are trivial, as $T_0(x) = 1$ and $T_1(x) = x$. For the inductive case, consider again even and odd, and assume that it holds for $n < k$.

                    For $k = 2t$ even, the leading coefficient $\mathcal L \mathcal C (T_k)$:
                    \begin{align*}
                        \mathcal L \mathcal C (T_k) &= 2 \mathcal L \mathcal C (T_t)^2 \text{ From the recurrence in (a)} \\
                        &= 2  (2^{t-1})^2 = 2 (2^{2t-2}) = 2^{k-1} \text{ By IH}\\
                    \end{align*}

                    For $k = 2t+1$ odd,
                    \begin{align*}
                        \mathcal L \mathcal C (T_k) &= \mathcal L \mathcal C (T_{k-1}) + \mathcal L \mathcal C (T_t) \mathcal L \mathcal C (T_{t+1}) \text{ From the other recurrence in (a)} \\
                        &= 2^{k-2} + (2^{t-1})(2^t) \text{ From IH} \\
                        &= 2^{k-2} + 2^{2t-1}\\
                        &= 2^{k-2} + 2^{k-2} = 2^{k-1}\\
                    \end{align*}

                \end{proof} 
            \item Derive the roots of (6) (i.e., the so-called Chebyshev points).
                
                The roots for a polynomial of degree $k$ are $x_i = \cos(\frac{(2i+1)\pi}{2k})$ where $i = 0,\dots k-1$.
                \begin{proof}
                    Given the definition for $T_k$, plugging in $x_i$ gives
                    \begin{align*}
                        \cos(k\arccos(x_i)) &= \cos(k\arccos (\cos\Big(\frac{(2i+1)\pi}{2k}\Big))) \\
                        &= \cos\Big(k\frac{(2i+1)\pi}{2k}\Big) \\
                        &= \cos\Big(\frac{(2i+1)\pi}{2}\Big) \\
                        &= 0 \, \text{ When }i \in \mathbb{N}
                    \end{align*} 
                \end{proof}  
            \item Complete the proof showing why the Chebyshev points are optimal.
        \end{enumerate}
    \newpage
    \item Consider the definite integral $I(f) = \int_{-1}^1 f(x) \,dx$.
        \begin{enumerate}
            \item Construct the interpolatory quadrature rule for this integral based on nodes $-1, -\frac{1}{2}, \frac{1}{2}, 1$.
            \item What is the precision of the quadrature rule derived in (a)? \textbf{Justify your answer.}
            \item Find as good an error bound as you can for your quadrature rule, assuming $f$ is as smooth as required for your analysis.
        \end{enumerate}
\end{enumerate}
\end{document}
